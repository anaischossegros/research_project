{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Loader import load_data\n",
    "from Train import trainCox_nnet\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "''' Net Settings'''\n",
    "Hidden_Nodes = 143 ###number of hidden nodes\n",
    "Out_Nodes = 30 ###number of hidden nodes in the last hidden layer\n",
    "''' Initialize '''\n",
    "Initial_Learning_Rate = [0.01, 0.001, 0.00075]\n",
    "L2_Lambda = [0.01, 0.005, 0.001]\n",
    "L1_Lambda = [0.01, 0.005, 0.001]\n",
    "num_epochs = 3 ###for pancreas\n",
    "Num_EPOCHS = 3 ###for lung\n",
    "###sub-network setup\n",
    "Dropout_Rate = [0.7, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_extraction_lung import data_norm_df_lung, output_df_lung\n",
    "\n",
    "data_norm_df_lung= data_norm_df_lung.reset_index(drop=True)\n",
    "output_df_lung = output_df_lung.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_extraction_pancreas import data_norm_df_pancreas, output_df_pancreas\n",
    "data_norm_df_pancreas= data_norm_df_pancreas.reset_index(drop=True)\n",
    "output_df_pancreas = output_df_pancreas.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_norm_df_lung,output_df_lung], axis=1)\n",
    "x, ytime, yevent, age = load_data(data, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation + Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "#Variance threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(0.2)\n",
    "x = selector.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Loader import CustomDataset\n",
    "\n",
    "batch_size=32\n",
    "data2 = CustomDataset(x, ytime, yevent, age)\n",
    "In_Nodes = len(x[0,:]) ###number of genes\n",
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for the optimal learning rate, regularisation l1 and L2, drop out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_l2_loss = 0\n",
    "opt_l1_loss = 0\n",
    "opt_lr_loss = 0\n",
    "opt_do_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "###if gpu is being used\n",
    "if torch.cuda.is_available():\n",
    "\topt_loss = opt_loss.cuda()\n",
    "###\n",
    "opt_c_index_va = 0\n",
    "opt_c_index_tr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l2 in L2_Lambda:\n",
    "\tfor lr in Initial_Learning_Rate:\n",
    "\t\tfor l1 in L1_Lambda:\n",
    "\t\t\thistory_train, history_val = trainCox_nnet(data2, \\\n",
    "\t\t\t\tIn_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\t\tlr, l2, l1, num_epochs, 0.5, batch_size)\n",
    "\t\t\tc_index_val_fold0 = [k['c_index'] for k in history_val[0]]\n",
    "\t\t\tc_index_val_fold1 = [k['c_index'] for k in history_val[1]]\n",
    "\t\t\tc_index=np.mean([c_index_val_fold0[-1],c_index_val_fold1[-1]])\n",
    "\t\t\tif c_index==0: \n",
    "\t\t\t\tbreak\n",
    "\t\t\telif c_index > opt_c_index_va:\n",
    "\t\t\t\topt_l2_loss = l2\n",
    "\t\t\t\topt_lr_loss = lr\n",
    "\t\t\t\topt_l1_loss = l1\n",
    "\t\t\t\topt_do_loss = 0.5\n",
    "\t\t\t\topt_c_index_va = c_index\n",
    "\t\t\t\t# opt_c_index_tr = c_index_tr\n",
    "\t\t\t\t# opt_c_index_va = c_index_va\n",
    "\t\t\tprint (\"L2: \", l2, \"L1:\", l1, \"LR: \", lr, \"c_index\", opt_c_index_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###train Cox-nnet with optimal hyperparameters using train data, and then evaluate the trained model with test data\n",
    "###Note that test data are only used to evaluate the trained Cox-nnet\n",
    "history_train, history_val = trainCox_nnet(data2, \\\n",
    "\t\t\tIn_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\topt_lr_loss, opt_l2_loss, opt_l1_loss, Num_EPOCHS, opt_do_loss, batch_size)\n",
    "print (\"Optimal L2: \", opt_l2_loss, \"Optimal LR: \", opt_lr_loss)\n",
    "# print(\"C-index in Test: \", c_index_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the accuracy and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_fold0 = [k['loss'] for k in history_train[0]]\n",
    "loss_val_fold0 = [k['loss'] for k in history_val[0]] \n",
    "c_index_train_fold0 = [k['c_index'] for k in history_train[0]]\n",
    "c_index_val_fold0 = [k['c_index'] for k in history_val[0]]\n",
    "\n",
    "\n",
    "loss_train_fold1 = [k['loss'] for k in history_train[1]]\n",
    "loss_val_fold1 = [k['loss'] for k in history_val[1]] \n",
    "c_index_train_fold1 = [k['c_index'] for k in history_train[1]]\n",
    "c_index_val_fold1 = [k['c_index'] for k in history_val[1]]\n",
    "\n",
    "loss_train_fold2 = [k['loss'] for k in history_train[2]]\n",
    "loss_val_fold2 = [k['loss'] for k in history_val[2]] \n",
    "c_index_train_fold2 = [k['c_index'] for k in history_train[2]]\n",
    "c_index_val_fold2 = [k['c_index'] for k in history_val[2]]\n",
    "\n",
    "loss_train_fold3 = [k['loss'] for k in history_train[3]]\n",
    "loss_val_fold3 = [k['loss'] for k in history_val[3]] \n",
    "c_index_train_fold3 = [k['c_index'] for k in history_train[3]]\n",
    "c_index_val_fold3 = [k['c_index'] for k in history_val[3]]\n",
    "\n",
    "loss_train_fold4 = [k['loss'] for k in history_train[4]]\n",
    "loss_val_fold4 = [k['loss'] for k in history_val[4]] \n",
    "c_index_train_fold4 = [k['c_index'] for k in history_train[4]]\n",
    "c_index_val_fold4 = [k['c_index'] for k in history_val[4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_fold5 = [k['loss'] for k in history_train[5]]\n",
    "loss_val_fold5 = [k['loss'] for k in history_val[5]] \n",
    "c_index_train_fold5 = [k['c_index'] for k in history_train[5]]\n",
    "c_index_val_fold5 = [k['c_index'] for k in history_val[5]]\n",
    "\n",
    "\n",
    "loss_train_fold6 = [k['loss'] for k in history_train[6]]\n",
    "loss_val_fold6 = [k['loss'] for k in history_val[6]] \n",
    "c_index_train_fold6 = [k['c_index'] for k in history_train[6]]\n",
    "c_index_val_fold6 = [k['c_index'] for k in history_val[6]]\n",
    "\n",
    "loss_train_fold7 = [k['loss'] for k in history_train[7]]\n",
    "loss_val_fold7 = [k['loss'] for k in history_val[7]] \n",
    "c_index_train_fold7 = [k['c_index'] for k in history_train[7]]\n",
    "c_index_val_fold7 = [k['c_index'] for k in history_val[7]]\n",
    "\n",
    "loss_train_fold8 = [k['loss'] for k in history_train[8]]\n",
    "loss_val_fold8 = [k['loss'] for k in history_val[8]] \n",
    "c_index_train_fold8 = [k['c_index'] for k in history_train[8]]\n",
    "c_index_val_fold8 = [k['c_index'] for k in history_val[8]]\n",
    "\n",
    "loss_train_fold9 = [k['loss'] for k in history_train[9]]\n",
    "loss_val_fold9 = [k['loss'] for k in history_val[9]] \n",
    "c_index_train_fold9 = [k['c_index'] for k in history_train[9]]\n",
    "c_index_val_fold9 = [k['c_index'] for k in history_val[9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(0,Num_EPOCHS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(221)\n",
    "plt.plot(epoch, loss_train_fold0, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold0, label = 'validation')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(epoch, loss_train_fold1, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold1, label = 'validation')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(epoch, loss_train_fold2, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold2, label = 'validation')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(epoch, loss_train_fold4, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold4, label = 'validation')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(221)\n",
    "plt.plot(epoch, c_index_train_fold0, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold0, label = 'validation')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(epoch, c_index_train_fold1, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold1, label = 'validation')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(epoch, c_index_train_fold3, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold3, label = 'validation')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(epoch, c_index_train_fold4, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold4, label = 'validation')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result cross validation\n",
    "\n",
    "print(np.mean([c_index_val_fold1[4],  c_index_val_fold0[4], c_index_val_fold3[4], c_index_val_fold2[4],c_index_val_fold4[4]]))\n",
    "print(np.std([c_index_val_fold1[4],  c_index_val_fold0[4], c_index_val_fold3[4], c_index_val_fold2[4],c_index_val_fold4[4]]))\n",
    "print(([c_index_val_fold1[4],  c_index_val_fold0[4], c_index_val_fold3[4], c_index_val_fold2[4],c_index_val_fold4[4]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result cross validation\n",
    "\n",
    "print(np.mean([c_index_val_fold0[6],  c_index_val_fold1[6], c_index_val_fold2[6]]))\n",
    "print(np.std([c_index_val_fold0[6],  c_index_val_fold1[6], c_index_val_fold2[6]]))\n",
    "print([c_index_val_fold0[6],  c_index_val_fold1[6], c_index_val_fold2[6]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_extraction_pancreas import data_norm_df_pancreas, output_df_pancreas\n",
    "data_norm_df_pancreas= data_norm_df_pancreas.reset_index(drop=True)\n",
    "output_df_pancreas = output_df_pancreas.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pancreas = pd.concat([data_norm_df_pancreas,output_df_pancreas], axis=1)\n",
    "x_pancreas, ytime_pancreas, yevent_pancreas, age_pancreas = load_data(data_pancreas, dtype)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_pancreas = StandardScaler().fit_transform(x_pancreas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Loader import CustomDataset\n",
    "batch_size=32\n",
    "data2_pancreas = CustomDataset(x_pancreas, ytime_pancreas, yevent_pancreas, age_pancreas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Cox_nnet\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "In_Nodes = 60660\n",
    "net = Cox_nnet(In_Nodes, Hidden_Nodes, Out_Nodes, 0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "from Survival_CostFunc_CIndex import R_set, neg_par_log_likelihood, c_index\n",
    "\n",
    "class Cox_nnet_transfer(nn.Module):\n",
    "    def __init__(self, In_Nodes, Hidden_Nodes, Out_Nodes, Cox_nnet, Dropout): \n",
    "        super(Cox_nnet_transfer, self).__init__()\n",
    "        self.trainer = nn.Sequential(*list(Cox_nnet.children())[:-1]) # strips off last linear layer from Cox_nnet\n",
    "        # for param in self.trainer.parameters(): \n",
    "        #     param.requires_grad = False\n",
    "        self.classifier = nn.Linear(Out_Nodes+1, 1, bias=False)\n",
    "        self.classifier.weight.data.uniform_(-0.001, 0.001)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x_1, x_2):\n",
    "        #Normal 1 layer\n",
    "        x_1=self.trainer(x_1)\n",
    "        x_cat = torch.cat((x_1, x_2), 1)\n",
    "        lin_pred=self.classifier(x_cat)\n",
    "        return lin_pred\n",
    "\n",
    "    def training_step(self, batch): \n",
    "        x_train_b, ytime_train_b, yevent_train_b, age_train_b = batch\n",
    "        # print(batch)\n",
    "        pred = self(x_train_b.float(), age_train_b) ###Forward\n",
    "        loss = neg_par_log_likelihood(pred, ytime_train_b, yevent_train_b) ###calculate loss\n",
    "        acc = c_index(pred, ytime_train_b, yevent_train_b) #calculate accuracy\n",
    "        return{'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def training_epoch_end(self, pred):\n",
    "        batch_losses = [x['val_loss'] for x in pred]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in pred]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return{'loss': epoch_loss.item(), 'c_index': epoch_acc.item()}\n",
    "\n",
    "    def validation_step(self, batch): \n",
    "        x_eval_b, ytime_eval_b, yevent_eval_b, age_eval_b = batch\n",
    "        eval_pred = self(x_eval_b.float(), age_eval_b)\n",
    "        loss = neg_par_log_likelihood(eval_pred, ytime_eval_b, yevent_eval_b)\n",
    "        # acc= concordance_index_censored(yevent_eval_b, ytime_eval_b,eval_pred)\n",
    "        acc = c_index(eval_pred, ytime_eval_b, yevent_eval_b)\n",
    "        return{'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, pred): \n",
    "        batch_losses = [x['val_loss'] for x in pred]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in pred]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return{'loss': epoch_loss.item(), 'c_index': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result): \n",
    "        print(\"Epoch [{}], loss: {:.4f}, c_index: {:.4f}\".format(epoch, result['loss'], result['c_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Cox_nnet_transfer(In_Nodes, Hidden_Nodes, Out_Nodes, net, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "\tfor layer in m.children():\n",
    "\t\tif hasattr(layer, 'reset_parameters'):\n",
    "\t\t\tlayer.reset_parameters()\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "history_val_pancreas=[[],[],[],[],[]]\n",
    "history_train_pancreas=[[],[],[],[],[]]\n",
    "for fold,(train_idx,test_idx) in enumerate(kfold.split(data2_pancreas)):\n",
    "    my_model = Cox_nnet_transfer(In_Nodes, Hidden_Nodes, Out_Nodes, net, 0.5)\n",
    "    opt = optim.Adam(my_model.parameters(), lr=opt_lr_loss, weight_decay = opt_l2_loss)\n",
    "    print('------------fold no---------{}----------------------'.format(fold))\n",
    "    train_loader = DataLoader(data2_pancreas, batch_size=15, sampler=train_idx)\n",
    "    val_loader = DataLoader(data2_pancreas, batch_size=15, sampler=test_idx)\n",
    "    # print(train_idx)\n",
    "    for epoch in range(num_epochs+1):\n",
    "        #training phase\n",
    "        pred_train=[]\n",
    "        for batch in train_loader: \n",
    "            loss = my_model.training_step(batch)\n",
    "            loss = loss['val_loss']\n",
    "            regularization_loss = 0\n",
    "            for param in net.parameters():\n",
    "                regularization_loss += torch.sum(abs(param))\n",
    "            loss = loss+0.001*regularization_loss\n",
    "            loss_batch_train.append(loss)\n",
    "            loss.backward() ###calculate gradientsloss = loss['val_loss']\n",
    "            opt.step() ###update weights and biases\n",
    "            opt.zero_grad() ###reset gradients to zeros\n",
    "            pred_train.append(my_model.training_step(batch))\n",
    "        result_train = my_model.training_epoch_end(pred_train)\n",
    "        pred_val = \t[my_model.validation_step(batch) for batch in val_loader]\n",
    "        result_val = my_model.validation_epoch_end(pred_val)\n",
    "        my_model.epoch_end(epoch, result_val)\n",
    "        history_val_pancreas[fold].append(result_val)\n",
    "        history_train_pancreas[fold].append(result_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = num_epochs+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_fold0_pancreas = [k['loss'] for k in history_train_pancreas[0]]\n",
    "loss_val_fold0_pancreas = [k['loss'] for k in history_val_pancreas[0]] \n",
    "c_index_train_fold0_pancreas = [k['c_index'] for k in history_train_pancreas[0]]\n",
    "c_index_val_fold0_pancreas = [k['c_index'] for k in history_val_pancreas[0]]\n",
    "\n",
    "loss_train_fold1_pancreas = [k['loss'] for k in history_train_pancreas[1]]\n",
    "loss_val_fold1_pancreas = [k['loss'] for k in history_val_pancreas[1]] \n",
    "c_index_train_fold1_pancreas = [k['c_index'] for k in history_train_pancreas[1]]\n",
    "c_index_val_fold1_pancreas = [k['c_index'] for k in history_val_pancreas[1]]\n",
    "\n",
    "loss_train_fold2_pancreas = [k['loss'] for k in history_train_pancreas[2]]\n",
    "loss_val_fold2_pancreas = [k['loss'] for k in history_val_pancreas[2]] \n",
    "c_index_train_fold2_pancreas = [k['c_index'] for k in history_train_pancreas[2]]\n",
    "c_index_val_fold2_pancreas = [k['c_index'] for k in history_val_pancreas[2]]\n",
    "\n",
    "loss_train_fold3_pancreas = [k['loss'] for k in history_train_pancreas[3]]\n",
    "loss_val_fold3_pancreas = [k['loss'] for k in history_val_pancreas[3]] \n",
    "c_index_train_fold3_pancreas = [k['c_index'] for k in history_train_pancreas[3]]\n",
    "c_index_val_fold3_pancreas = [k['c_index'] for k in history_val_pancreas[3]]\n",
    "\n",
    "loss_train_fold4_pancreas = [k['loss'] for k in history_train_pancreas[4]]\n",
    "loss_val_fold4_pancreas = [k['loss'] for k in history_val_pancreas[4]] \n",
    "c_index_train_fold4_pancreas = [k['c_index'] for k in history_train_pancreas[4]]\n",
    "c_index_val_fold4_pancreas = [k['c_index'] for k in history_val_pancreas[4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(0,num_epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(221)\n",
    "plt.plot(epoch, loss_train_fold4_pancreas, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold4_pancreas, label = 'testing')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(epoch, loss_train_fold1_pancreas, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold1_pancreas, label = 'testing')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(epoch, loss_train_fold2_pancreas, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold2_pancreas, label = 'testing')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(epoch, loss_train_fold0_pancreas, label = 'training')\n",
    "plt.plot(epoch, loss_val_fold0_pancreas, label = 'testing')\n",
    "plt.ylabel('loss = log likelihood')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(221)\n",
    "plt.plot(epoch, c_index_train_fold0_pancreas, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold0_pancreas, label = 'testing')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(epoch, c_index_train_fold1_pancreas, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold1_pancreas, label = 'testing')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(epoch, c_index_train_fold3_pancreas, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold3_pancreas, label = 'testing')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(epoch, c_index_train_fold4_pancreas, label = 'training')\n",
    "plt.plot(epoch, c_index_val_fold4_pancreas, label = 'testing')\n",
    "plt.ylabel('acc= c_index')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([c_index_val_fold0_pancreas[1],c_index_val_fold1_pancreas[1],c_index_val_fold2_pancreas[1],c_index_val_fold3_pancreas[1],c_index_val_fold4_pancreas[1]]))\n",
    "print(([c_index_val_fold0_pancreas[1],c_index_val_fold1_pancreas[1],c_index_val_fold2_pancreas[1],c_index_val_fold3_pancreas[1],c_index_val_fold4_pancreas[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb288fa2e0b97d7fe5d36e6cd017a998fc9aeeeb66a354179c06f074395a5c37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
